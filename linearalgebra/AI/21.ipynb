{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数在机器学习中的应用\n",
    "在机器学习中，数据往往以矩阵的形式存在。我们用矩阵来代表特征、样本和权重等。\n",
    "\n",
    "- 向量：数据的基本单位，通常表示为一列数值的数组。\n",
    "- 矩阵：由向量组成的二维数组，用于表示多个样本与特征间的关系。\n",
    "- 转置：矩阵的转置操作可以将行与列进行互换，标记为 $ A^T $。\n",
    "- 内积与外积：内积用于计算两个向量的相似度，外积则在构建矩阵时常用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归与线性代数的关系\n",
    "线性回归是最基础的机器学习模型之一，其目标是通过已知的数据点来拟合一个线性方程来预测输出。其模型可以表示为：\n",
    "$$ y = X \\beta + \\epsilon $$\n",
    "其中，$ y $ 是目标变量（输出），$ X $是特征矩阵，$ \\beta $ 是参数向量，$ \\epsilon $是误差项。\n",
    "在训练过程中，我们通过最小化损失函数来求解最佳的参数 $ \\beta $。损失函数可以表示为：\n",
    "$$ L(\\beta) = || y - X \\beta ||^2 $$\n",
    "使用线性代数的方法，我们可以通过求解正规方程：\n",
    "$$ \\beta = (X^T X)^{-1} X^T y $$\n",
    "这是利用矩阵的运算来直接得到线性回归模型的参数，实现运算的高效性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主成分分析(Principal Component Analysis, PCA)\n",
    "主成分分析是一种降维技术，它帮助我们提取数据中的主要特征。PCA的核心思想是通过线性代数对数据进行变换，使高维数据在低维空间中呈现出较好的结构。\n",
    "\n",
    "PCA相关的步骤如下：\n",
    "\n",
    "1. 标准化数据：计算每个特征的均值和标准差，将数据标准化为均值为0，方差为1。\n",
    "2. 计算协方差矩阵：通过样本特征的协方差来反映特征之间的关系。\n",
    "$$ Cov(X) = \\frac{1}{n-1} X^T X $$\n",
    "3. 特征值分解：对协方差矩阵进行特征值分解，得到特征值和特征向量。\n",
    "4. 选择主成分：选择前k个特征值对应的特征向量，构成新的特征空间。\n",
    "\n",
    "#### 例子\n",
    "在这个示例中，我们通过PCA将二维数据降维到一维，提取了主要成分。这样的操作可以帮助我们在处理高维数据时减少计算复杂度，提高模型的训练效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维后的数据：\n",
      "[[ 0.50867755]\n",
      " [-2.09784025]\n",
      " [ 0.67118032]\n",
      " [-0.04602092]\n",
      " [ 1.35661069]\n",
      " [ 0.95911985]\n",
      " [ 1.28859716]\n",
      " [-0.41785941]\n",
      " [-1.46453797]\n",
      " [-0.75792702]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成样本数据\n",
    "data = np.array([[2.5, 2.4],\n",
    "                 [0.5, 0.7],\n",
    "                 [2.2, 2.9],\n",
    "                 [1.9, 2.2],\n",
    "                 [3.1, 3.0],\n",
    "                 [2.3, 3.2],\n",
    "                 [3.0, 3.0],\n",
    "                 [2.0, 1.6],\n",
    "                 [1.0, 1.1],\n",
    "                 [1.5, 1.6]])\n",
    "\n",
    "# 标准化\n",
    "data_meaned = data - np.mean(data, axis=0)\n",
    "\n",
    "# 计算协方差矩阵\n",
    "cov_mat = np.cov(data_meaned, rowvar=False)\n",
    "\n",
    "# 特征值分解\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "# 选择前k个特征向量\n",
    "k = 1\n",
    "top_k_eigenvectors = eigenvectors[:, -k:]\n",
    "\n",
    "# 将数据转换到新的特征空间\n",
    "reduced_data = np.dot(data_meaned, top_k_eigenvectors)\n",
    "\n",
    "print(\"降维后的数据：\")\n",
    "print(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树中的线性代数应用\n",
    "尽管决策树本身并不直接依赖线性代数，但在决策树的特征选择与分裂过程中，信息增益和基尼指数等指标的计算也与线性代数密切相关。我们需要对数据进行划分，并计算每个区间的均值和方差，从而决定最佳划分点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数与神经网络\n",
    "深度学习中的核心构建块是神经网络，而神经网络可以使用矩阵和向量的运算来表示。一个简单的前馈神经网络能够通过线性变换（例如矩阵乘法）和非线性激活函数（例如ReLU、Sigmoid等）来学习复杂的函数关系。\n",
    "\n",
    "#### 线性变换\n",
    "在一个典型的深度神经网络中，输入数据（通常是特征向量）会通过多个隐藏层。每一层都可以用线性变换（矩阵乘法）加偏置来表示，形式如下：\n",
    "$$ \\mathbf{z} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b} $$\n",
    "其中，$ \\mathbf{z} $ 是下一层的输入，$ \\mathbf{W} $ 是权重矩阵，$ \\mathbf{x} $ 是当前层的输入，$ \\mathbf{b} $是偏置向量。\n",
    "\n",
    "一个输入层有 3 个神经元，一个隐藏层有 2 个神经元的网络，我们可以表示为：\n",
    "$$ \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} \n",
    "= \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix} \n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \n",
    "+ \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} $$\n",
    "这一操作清晰地展示了输入与权重之间的关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非线性激活\n",
    "在计算出线性变换的结果之后，通常会应用一个非线性激活函数，增强模型的表达能力。这一过程如下：\n",
    "$$ \\mathbf{a} = f(\\mathbf{z}) $$\n",
    "\n",
    "这里，$ f $ 表示激活函数，例如 ReLU 或 Sigmoid。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播\n",
    "在深度学习中，训练神经网络通常使用反向传播算法来优化权重和偏置。反向传播需要计算损失函数相对于每个权重和偏置的梯度。这个过程涉及到大量的矩阵运算和向量运算，使用了线性代数中的导数和链式法则。\n",
    "\n",
    "例如，设损失函数为 $ L $ ，我们可以使用链式法则得到权重更新的公式：\n",
    "$$ \\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\cdot \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例子**\n",
    "用一个三层神经网络来分类手写数字（例如 MNIST 数据集）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output:\n",
      " [[0.89742745]\n",
      " [0.89204576]\n",
      " [0.90282371]\n",
      " [0.89698118]\n",
      " [0.89560005]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 激活函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 前向传播\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    output = sigmoid(z2)\n",
    "    return output\n",
    "\n",
    "# 示例输入\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(5, 3)  # 5 个样本，3 个特征的输入\n",
    "W1 = np.random.rand(3, 4)  # 第一层权重 3->4\n",
    "b1 = np.random.rand(4)      # 第一层偏置\n",
    "W2 = np.random.rand(4, 1)  # 第二层权重 4->1\n",
    "b2 = np.random.rand(1)      # 第二层偏置\n",
    "\n",
    "output = forward(X, W1, b1, W2, b2)\n",
    "print(\"Network output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，我们首先生成一些随机输入数据 $ X $ ，然后通过指定的权重和偏置进行前向传播，最终得到网络的输出。通过调整 \n",
    "$ W_1, b_1, W_2, b_2 $，我们可以训练模型，以便它可以更好地分类手写数字。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
